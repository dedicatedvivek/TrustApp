<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 3.2//EN">
<!-- saved from url=(0079)http://homepages.inf.ed.ac.uk/rbf/CVonline/LOCAL_COPIES/OWENS/LECT11/node5.html -->
<html><head><meta http-equiv="Content-Type" content="text/html; charset=windows-1252">
<title>Stereo matching</title>
<meta name="description" content="Stereo matching">
<meta name="keywords" content="lect11">
<meta name="resource-type" content="document">
<meta name="distribution" content="global">

<link rel="STYLESHEET" href="http://homepages.inf.ed.ac.uk/rbf/CVonline/LOCAL_COPIES/OWENS/LECT11/lect11.css">
<link rel="next" href="http://homepages.inf.ed.ac.uk/rbf/CVonline/LOCAL_COPIES/OWENS/LECT11/node6.html">
<link rel="previous" href="http://homepages.inf.ed.ac.uk/rbf/CVonline/LOCAL_COPIES/OWENS/LECT11/node4.html">
<link rel="up" href="http://homepages.inf.ed.ac.uk/rbf/CVonline/LOCAL_COPIES/OWENS/LECT11/lect11.html">
<link rel="next" href="http://homepages.inf.ed.ac.uk/rbf/CVonline/LOCAL_COPIES/OWENS/LECT11/node6.html">
</head>
<body>
<!--Navigation Panel-->
<a name="tex2html98" href="http://homepages.inf.ed.ac.uk/rbf/CVonline/LOCAL_COPIES/OWENS/LECT11/node6.html">
<img width="37" height="24" align="BOTTOM" border="0" alt="next" src="./Stereo matching_files/next_motif.gif"></a> 
<a name="tex2html96" href="http://homepages.inf.ed.ac.uk/rbf/CVonline/LOCAL_COPIES/OWENS/LECT11/lect11.html">
<img width="26" height="24" align="BOTTOM" border="0" alt="up" src="./Stereo matching_files/up_motif.gif"></a> 
<a name="tex2html90" href="http://homepages.inf.ed.ac.uk/rbf/CVonline/LOCAL_COPIES/OWENS/LECT11/node4.html">
<img width="63" height="24" align="BOTTOM" border="0" alt="previous" src="./Stereo matching_files/previous_motif.gif"></a>   
<br>
<b> Next:</b> <a name="tex2html99" href="http://homepages.inf.ed.ac.uk/rbf/CVonline/LOCAL_COPIES/OWENS/LECT11/node6.html">Some existing matching algorithms</a>
<b> Up:</b> <a name="tex2html97" href="http://homepages.inf.ed.ac.uk/rbf/CVonline/LOCAL_COPIES/OWENS/LECT11/lect11.html">Computer Vision IT412</a>
<b> Previous:</b> <a name="tex2html91" href="http://homepages.inf.ed.ac.uk/rbf/CVonline/LOCAL_COPIES/OWENS/LECT11/node4.html">Problem definition</a>
<br>
<br>
<!--End of Navigation Panel-->
<!--Table of Child-Links-->
<a name="CHILD_LINKS"><strong>Subsections</strong></a>
<ul>
<li><a name="tex2html100" href="http://homepages.inf.ed.ac.uk/rbf/CVonline/LOCAL_COPIES/OWENS/LECT11/node5.html#SECTION00051000000000000000">
Intensity-based stereo matching</a>
</li><li><a name="tex2html101" href="http://homepages.inf.ed.ac.uk/rbf/CVonline/LOCAL_COPIES/OWENS/LECT11/node5.html#SECTION00052000000000000000">
Feature-based stereo matching</a>
<ul>
<li><a name="tex2html102" href="http://homepages.inf.ed.ac.uk/rbf/CVonline/LOCAL_COPIES/OWENS/LECT11/node5.html#SECTION00052100000000000000">
Types of features</a>
</li></ul>
</li><li><a name="tex2html103" href="http://homepages.inf.ed.ac.uk/rbf/CVonline/LOCAL_COPIES/OWENS/LECT11/node5.html#SECTION00053000000000000000">
Matching constraints</a>
</li><li><a name="tex2html104" href="http://homepages.inf.ed.ac.uk/rbf/CVonline/LOCAL_COPIES/OWENS/LECT11/node5.html#SECTION00054000000000000000">
Coarse-to-fine multiresolution matching scheme</a>
</li></ul>
<!--End of Table of Child-Links-->
<hr>
<h1><a name="SECTION00050000000000000000">
Stereo matching</a>
</h1>
<a name="sec:match">&nbsp;</a>Approaches to the correspondence problem can be broadly
classified into two categories: the intensity-based matching
and the feature-based matching techniques.
In the first category, the matching process is applied directly
to the intensity profiles of the two images, while in the
second, features are first extracted from the images and
the matching process is applied to the features.
<p></p><h2><a name="SECTION00051000000000000000">
Intensity-based stereo matching</a>
</h2>
<a name="sec:inten-match">&nbsp;</a>As shown in the previous section, the epipolar lines
coincide with the horizontal scanlines if the cameras
are parallel, the corresponding points in both images
must therefore lie on the same horizontal scanline.
Such stereo configurations reduce the search for
correspondences from two-dimensions (the entire image)
to one-dimension.  In fact, a close look at the intensity
profiles from the corresponding row of the image pair
reveals that the two intensity profiles differ only
by a horizontal shift and a local foreshortening.
Fig.&nbsp;<a href="http://homepages.inf.ed.ac.uk/rbf/CVonline/LOCAL_COPIES/OWENS/LECT11/node5.html#fig:inten-prof">5</a>(a) and&nbsp;(b) depict the
images taken with a camera that undergoes a displacement
in the horizontal direction, the image pair therefore corresponds
to a parallel camera set up.
Two black lines are marked at rows 80 and 230 in both images.
Fig.&nbsp;<a href="http://homepages.inf.ed.ac.uk/rbf/CVonline/LOCAL_COPIES/OWENS/LECT11/node5.html#fig:inten-prof">5</a>(c) and&nbsp;(d), (e) and&nbsp;(f), respectively
show the intensity
profiles of row 80 and row 230 of the two images.
<p>
<br>
</p><div align="CENTER"><a name="fig:inten-prof">&nbsp;</a><a name="108">&nbsp;</a>
<table>
<caption><strong>Figure 5:</strong>
A comparison of the intensity profiles of two
	images from a parallel stereo configuration.
	The image pair shown above is retrieved via ftp from
	the Calibrated Imaging Laboratory of CMU.</caption>
<tbody><tr><td><img width="487" height="689" src="./Stereo matching_files/img10.gif" alt="\begin{figure}
\begin{center}
\
\begin{minipage}
{5cm}
(a) left image \\ 
\psfig...
 ...igure=inten_prof/plotrig230.ps,width=5cm}
\end{minipage}\end{center}\end{figure}"></td></tr>
</tbody></table>
</div>
<br>
<p>
The similarity between the one-dimensional intensity profiles
of the two images suggests an optimization process would be
suitable.  Indeed, Barn in 1987 attempted matching
the parallel stereo images using simulated annealing.
He defined an energy function <i>E</i><sub><i>ij</i></sub> as:

</p><p align="CENTER"><img width="405" height="32" src="./Stereo matching_files/img11.gif" alt="\begin{displaymath}
E_{ij} = \vert I_L\left(i,j\right)-I_R\left(i,j+D(i,j)\right) \vert
 + \lambda \vert\bigtriangledown D(i,j)\vert\end{displaymath}"></p>
where <img width="62" height="33" align="MIDDLE" border="0" src="./Stereo matching_files/img12.gif" alt="$I_L\left(i,j\right)$"> denotes
	the intensity value of the left image
	at the <i>i</i>-th row and <i>j</i>-th column and
	<img width="65" height="33" align="MIDDLE" border="0" src="./Stereo matching_files/img13.gif" alt="$I_R\left(i,k\right)$"> denotes the intensity value
	of the right image at
	the same row but at the <i>k</i>-th column;
	<i>D</i>(<i>i</i>,<i>j</i>) is the disparity value (or <em>horizontal shift</em>
	in this case) at the <i>ij</i>-position of the left image.
<p>
The above is clearly a constrained optimization problem in
which the only constraint being used is a minimum change of
disparity values <img width="102" height="33" align="MIDDLE" border="0" src="./Stereo matching_files/img14.gif" alt="$D(i,j), \,\forall i, j$">.  This constraint
is commonly known as the <em>continuity constraint</em>
(see Section&nbsp;<a href="http://homepages.inf.ed.ac.uk/rbf/CVonline/LOCAL_COPIES/OWENS/LECT11/node5.html#sec:constraints"><img align="BOTTOM" border="1" alt="[*]" src="./Stereo matching_files/cross_ref_motif.gif"></a>).
</p><p>
Robe later incorporated the use of a multiresolution scheme
(see Section&nbsp;<a href="http://homepages.inf.ed.ac.uk/rbf/CVonline/LOCAL_COPIES/OWENS/LECT11/node5.html#sec:c-to-f"><img align="BOTTOM" border="1" alt="[*]" src="./Stereo matching_files/cross_ref_motif.gif"></a>) together with a smoothness constraint similar
to that of Barn into the constrained optimization
process.  In addition to the horizontal shift of corresponding
pixels, they also allowed the corresponding pixels to undergo vertical
shift (i.e. disparity in the vertical direction), so their
matching method is not restricted to only parallel stereo images.
The energy function to be minimized, as expected, is more complicated
than the one given above.
</p><p>
The advantage of this intensity profile matching is that
a dense disparity map and, consequently a dense depth (or range) map,
is output.  Unfortunately, like all constrained optimization problems,
whether the system would converge to the global minima is
still an open problem, although, as reported by Robe92,
the multiresolution scheme, to a certain extent, helped
speed up convergence and avoid local minima.
</p><p>
An alternative approach in intensity-based stereo matching,
commonly known as the <em>window-based method</em>, is to
only match those regions in the images that are ``interesting'',
for instance, regions that contain high variation of intensity
values in the horizontal, vertical, and diagonal directions.
The simple Moravec's interest operator (1979)  
detects such regions (correspond to regions that have grey-level corners)
from the image pair, and
it has been widely used in many stereo matching systems
(e.g. the SRI STEREOSYS system (Hann, 1985)).
After the interesting regions are detected, a simple
correlation scheme is applied in the matching process;
a match is assigned to 
regions that are highly correlated in the two images.
</p><p>
The problem associated with this window-based approach
is that the size of the correlation windows must be
carefully chosen.  If the correlation windows are too small,
the intensity variation in the windows will not be
distinctive enough, and many false matches may result.
If they are too large, resolution is lost, since neighbouring
image regions with different disparities will be combined
in the measurement.  Worse, the two windows may not
correlate unless the disparity within the windows is
constant, which suggests that the multiresolution scheme
is again appropriate.
Unfortunately, the most serious shortcoming of the window-based approach
-- its sensitivity to the differences in foreshortening --
may sometimes render the approach useless.
Fig.&nbsp;<a href="http://homepages.inf.ed.ac.uk/rbf/CVonline/LOCAL_COPIES/OWENS/LECT11/node5.html#fig:foreshorten">6</a> shows a segment <i>MN</i> in the scene
<br>
</p><div align="CENTER"><a name="fig:foreshorten">&nbsp;</a><a name="122">&nbsp;</a>
<table>
<caption><strong>Figure 6:</strong>
Foreshortening due to the change of viewing position and
	direction.</caption>
<tbody><tr><td><img width="281" height="224" src="./Stereo matching_files/img15.gif" alt="\begin{figure}
\begin{center}
\

\psfig {figure=foreshorten.xfigps,height=5cm}
\end{center}\end{figure}"></td></tr>
</tbody></table>
</div>
<br>
projecting onto <img width="16" height="16" align="BOTTOM" border="0" src="./Stereo matching_files/img1.gif" alt="$\Pi$"> and <img width="21" height="16" align="BOTTOM" border="0" src="./Stereo matching_files/img2.gif" alt="$\Pi&#39;$"> at segments <img width="30" height="15" align="BOTTOM" border="0" src="./Stereo matching_files/img16.gif" alt="$\overline{mn}$">and <img width="39" height="18" align="BOTTOM" border="0" src="./Stereo matching_files/img17.gif" alt="$\overline{m&#39;n&#39;}$">, respectively.
Because of the large difference between the
orientations of the retinal planes and the scene plane,
segment <img width="30" height="15" align="BOTTOM" border="0" src="./Stereo matching_files/img16.gif" alt="$\overline{mn}$"> is much
longer than segment <img width="39" height="18" align="BOTTOM" border="0" src="./Stereo matching_files/img17.gif" alt="$\overline{m&#39;n&#39;}$">.The windows in the two images
would only give the best correlation measure
if the window used in <img width="16" height="16" align="BOTTOM" border="0" src="./Stereo matching_files/img1.gif" alt="$\Pi$"> has the size of <img width="30" height="15" align="BOTTOM" border="0" src="./Stereo matching_files/img16.gif" alt="$\overline{mn}$">and the window used in <img width="21" height="16" align="BOTTOM" border="0" src="./Stereo matching_files/img2.gif" alt="$\Pi&#39;$"> has the size of <img width="39" height="18" align="BOTTOM" border="0" src="./Stereo matching_files/img17.gif" alt="$\overline{m&#39;n&#39;}$">.Such variation of window size to compensate foreshortening
is not possible without
the knowledge of the scene.
This appears to pose a chicken-and-egg problem, since
the whole point is to recover shape using correlation matching.
<p></p><h2><a name="SECTION00052000000000000000">
Feature-based stereo matching</a>
</h2>
<a name="sec:feat-match">&nbsp;</a>In the feature-based approach, the image pair is first
<em>preprocessed</em> by an operator so as to extract
the <em>features</em> that are stable under the change
of viewpoint, the matching process is then applied
to the <em>attributes</em> associated with the detected features.
The obvious question here is what type of features that
one should use?
Edge elements, corners, line segments, and curve segments
are features that are robust against the change of perspective,
and they have been widely used in many stereo vision work.
Edge elements and corners
are easy to detect, but may suffer from occlusion;
line and curve segments require extra computation
time, but are more robust against occlusion (they are
longer and so are less likely to be completely occluded).  Higher level
image features such as circles, ellipses, and polygonal regions
have also been used as features for stereo matching,
these features are, however, restricted to images of
indoor scenes.
<p>
Most feature-based stereo matching systems are
not restricted to using only a specific type of features,
instead, a collection of feature types is incorporated.
For instance, the system proposed by Weng in 1988
combines intensity, edges, and corners to form multiple
attributes for matching; Lim and Binford (1987), 
on the other hand, used a hierarchy of features varying from
edges, curves, to surfaces and bodies (2-D regions)
for high-level attribute matching.
</p><p></p><h3><a name="SECTION00052100000000000000">
Types of features</a>
</h3>
<a name="sec:feat-type">&nbsp;</a><ul>
<li> <b>edge elements:</b>
	There exist many edge operators for finding
	edge elements from an image.
	For example, the <img width="42" height="34" align="MIDDLE" border="0" src="./Stereo matching_files/img18.gif" alt="$\bigtriangledown^2 G$">	operator followed by a detection of zero-crossings;
	the Canny edge detector (1986).
<p>
The attributes of edge elements used for matching
	can be: coordinates (location in the image),
	local orientations (or directions), local intensity profile
	on either side of the edge elements
	(e.g. from dark to light, or from light to dark).
</p></li><li> <b>corners:</b>
	The earliest corner detector is probably
	Beaudet's (1978) 
	rotationally invariant operator called <em>DET</em>;
	corner detectors reported in the 80s include
	Dreschler and Nagel (1982), 
	Kitchen and Rosenfeld (1982), 
	Zuniga and Haralick (1983),  etc.
	The Harris corner detector (1988) 
	is one of the popular corner detectors
	that are widely used today in Oxford and INRIA.
<p>
Attributes of corners that can be used for matching:
		coordinates of corners,
		type of junctions that the corners correspond to
		(e.g. Y-junction, L-function, A-junction, etc.).
</p></li><li> <b>line segments:</b>
	To extract line segments from an image, an
	edge operator must first be applied.
	Line segments are then formed by a linking
	and merging operation on the detected edge elements
	based on some criteria such as distance, similarity, and
	collinearity measures.
	Line segment extraction algorithms that have been
	reported include:
	Nevatia and Babu (1980), 
	Fischler and Bolles (1983), 
	Weiss and Boldt (1986).
<p>
Attributes of line segments that can be used for matching:
	coordinates of end-points, mid-points, orientation
	of line segments.
</p><p>
It should be noted that, due to image noise,
	the end-points (and thus the mid-points also)
	of line segments are normally not reliably
	detected, stereo matching process that relies
	on the coordinates of these points do not
	produce good reconstruction of 3-D coordinates.
	In fact, for a pair of matching line segments,
	any point on the first line segment can correspond
	to every other point on the second line segment,
	and this ambiguity can only be resolved if
	the end-points of the two line segments are
	known exactly.
	Using line segments as features for matching
	also has the following drawbacks:
	</p><ul>
<li> only those line segments that have a
		significant difference in direction
		with the epipolar lines can be matched.
		For line segments whose directions
		are close to those of the epipolar lines,
		any translation of the line segments
		along the line segments' directions
		does not cause a significant change to the
		epipolar geometry, yet the
		subsequent reconstruction process
		would be greatly affected.
</li><li> for a given line segment in one image,
		its corresponding line segment is
		likely to be broken into two or
		more shorter line segments, thus
		the uniqueness constraint
		(see Section&nbsp;<a href="http://homepages.inf.ed.ac.uk/rbf/CVonline/LOCAL_COPIES/OWENS/LECT11/node5.html#sec:constraints"><img align="BOTTOM" border="1" alt="[*]" src="./Stereo matching_files/cross_ref_motif.gif"></a>)
		may not be applicable.
</li><li> the reconstruction of 3-D lines
		from corresponding 2-D line segments
		is not well-defined if the end-points
		of the 2-D line segments are not reliable.
		This problem can be partially overcome by
		incorporating an uncertainty measure
		to the 2-D line segments' end-points
		as suggested by Zhang (1995). 
	</li></ul>
</li><li> <b>curve segments:</b>
	The matching of curve segments has not been widely
	attempted, the reason is probably due to
	the ambiguity involved -- every point on a curve
	is likely to be matchable with every other point
	on another curve.  
	Deriche and Faugeras' work (1990) 
	is one of the very few that has been reported.
	They proposed to match the turning points of curves.
</li><li> <b>circles, ellipses:</b>
	These features are present mainly in indoor scenes
	and applicable to detection of defects on industrial parts.
<p>
Attributes that can be used for matching: areas in pixel units,
	coordinates of the centre of the geometric figures.
</p></li><li> <b>regions:</b>
	Regions can be either defined as blobs (e.g. detected
	by a region growing algorithm) in the
	image or defined as polygonal regions bounded by
	line segments.
	Regions in the form of blobs have irregular boundary
	and may not match perfectly with regions from another image.
<p>
For polygonal regions, attributes that can be used for
	matching include: areas of regions, bounding line segments
	of regions, locations of regions' centroids.
</p><p>
Polygonal regions are very high-level features and could be
	costly to extract.
</p></li></ul><h2><a name="SECTION00053000000000000000">
Matching constraints</a>
</h2>
<a name="sec:constraints">&nbsp;</a>Stereo matching process is a very difficult search procedure.
In order to minimum false matches, some matching constraints
must be imposed.  Below is a list of the commonly used constraints.
<ul>
<li> <b>Similarity</b> (or <b>compatibility</b> Grimson, 1981).
	For the intensity-based approach, the matching pixels
	must have similar intensity values (i.e. differ lower than
	a specified threshold) or the matching windows must be
	highly correlated.
	For the feature-based approach, the matching features
	must have similar attribute values.
</li><li> <b>Uniqueness</b> Marr and Poggio, 1979.
	Almost always, a given pixel or feature from one
	image can match <em>no more than one</em> pixel or feature
	from the other image.
<p>
As noted earlier, the uniqueness constraint may not
	be applicable to the line segment-based approach.
	This constraint can also fail if transparent objects
	are present in the scene.  Furthermore, given a
	pixel or feature <i>m</i> in one image, its ``corresponding''
	pixel or feature may be occluded in the other image.
	In this case, no match should be assigned to <i>m</i>.
</p></li><li> <b>Continuity</b> Marr and Poggio, 1979.
	The cohesiveness of matters suggests that
	the disparity of the matches should vary smoothly almost
	everywhere over the image.
<p>
This constraint fails at discontinuities of depth,
	for depth discontinuities cause an abrupt change in
	disparity.
</p></li><li> <b>Ordering</b> Baker and Binford, 1981.
        If <img width="69" height="16" align="BOTTOM" border="0" src="./Stereo matching_files/img6.gif" alt="$m \leftrightarrow m&#39;$"> and <img width="59" height="16" align="BOTTOM" border="0" src="./Stereo matching_files/img19.gif" alt="$n \leftrightarrow n&#39;$"> and 
        if <i>m</i> is <em>to the left</em> of <i>n</i> 
        then <i>m</i>' should also be <em>to the left</em> of <i>n</i>'
        and vice versa.  That is, the ordering of features
	is preserved across images.
<p>
The ordering constraint fails at regions known
	as <em>the forbidden zone</em> (Fig.&nbsp;<a href="http://homepages.inf.ed.ac.uk/rbf/CVonline/LOCAL_COPIES/OWENS/LECT11/node5.html#fig:forbidden">7</a>).
<br>
</p><div align="CENTER"><a name="fig:forbidden">&nbsp;</a><a name="172">&nbsp;</a>
<table>
<caption><strong>Figure 7:</strong>
The ordering constraint fails if a given 3-D point
	(<i>N</i> here) falls onto the forbidden zone of another 3-D point
	(<i>M</i>). In the left image (<img width="16" height="16" align="BOTTOM" border="0" src="./Stereo matching_files/img1.gif" alt="$\Pi$">), <i>m</i> is to the right of <i>n</i>, but
	in the right image (<img width="21" height="16" align="BOTTOM" border="0" src="./Stereo matching_files/img2.gif" alt="$\Pi&#39;$">), this ordering is reversed.</caption>
<tbody><tr><td><img width="260" height="270" src="./Stereo matching_files/img20.gif" alt="\begin{figure}
\begin{center}
\

\psfig {figure=forbidden.xfigps,height=6cm}
\end{center}\end{figure}"></td></tr>
</tbody></table>
</div>
<br>
</li><li> <b>Epipolar</b>.
        Given a feature point <i>m</i> in the left image,
        the corresponding feature point <i>m</i>' must lie on the
        corresponding epipolar line.
<p>
This constraint reduces the search space from
        two-dimensions to one-dimension.  Unlike
	all the other constraints, the epipolar constraint
	would never fail and could be applied
	reliably once the epipolar geometry is known 
	(this is possible after the fundamental matrix
	is estimated, see Appendix&nbsp;<a href="http://homepages.inf.ed.ac.uk/rbf/CVonline/LOCAL_COPIES/OWENS/LECT11/node12.html#app:eplines">B</a>).
</p><p>
By introducing one more camera into the system,
	the ambiguity involved in matching can
	be further reduced:
	Given a feature point <img width="56" height="30" align="MIDDLE" border="0" src="./Stereo matching_files/img21.gif" alt="$m \in \Pi$">	and potential matches <img width="65" height="32" align="MIDDLE" border="0" src="./Stereo matching_files/img22.gif" alt="$m&#39; \in \Pi&#39;$"> and <img width="72" height="32" align="MIDDLE" border="0" src="./Stereo matching_files/img23.gif" alt="$m&#39;&#39; \in \Pi&#39;&#39;$">,	an epipolar line <img width="68" height="32" align="MIDDLE" border="0" src="./Stereo matching_files/img24.gif" alt="${\bf l}_{13} \in \Pi&#39;&#39;$"> can be constructed 
	using <i>m</i> and the epipolar geometry between <img width="16" height="16" align="BOTTOM" border="0" src="./Stereo matching_files/img1.gif" alt="$\Pi$"> and <img width="25" height="16" align="BOTTOM" border="0" src="./Stereo matching_files/img25.gif" alt="$\Pi&#39;&#39;$">,	another epipolar line <img width="68" height="32" align="MIDDLE" border="0" src="./Stereo matching_files/img26.gif" alt="${\bf l}_{23} \in \Pi&#39;&#39;$">	can also be constructed
	using <i>m</i>' and the epipolar geometry between <img width="21" height="16" align="BOTTOM" border="0" src="./Stereo matching_files/img2.gif" alt="$\Pi&#39;$"> and <img width="25" height="16" align="BOTTOM" border="0" src="./Stereo matching_files/img25.gif" alt="$\Pi&#39;&#39;$">.	The two epipolar lines <img width="23" height="30" align="MIDDLE" border="0" src="./Stereo matching_files/img27.gif" alt="${\bf l}_{13}$"> and <img width="23" height="30" align="MIDDLE" border="0" src="./Stereo matching_files/img28.gif" alt="${\bf l}_{23}$">	must intersect at <i>m</i>'' if
	<img width="124" height="16" align="BOTTOM" border="0" src="./Stereo matching_files/img29.gif" alt="$m \leftrightarrow m&#39; \leftrightarrow m&#39;&#39;$">.</p></li><li> <b>Relaxation</b>.
        A global matching constraint to eliminate false matches.
<p>
An example (Barnard and Thompson, (1980)) 
	of applying relaxation to stereo matching:
        assign to each candidate match a probability value
        based on some criteria on the ``goodness of match'';
        iteratively update this probability value for
        each candidate match; finally, delete those matches
        whose probability value is below a certain threshold.
	The update process in each iteration is as follows:
        increase probability of a given candidate match <img width="69" height="16" align="BOTTOM" border="0" src="./Stereo matching_files/img6.gif" alt="$m \leftrightarrow m&#39;$">        by a value that is proportional
        to the number of neighbouring matches that have
        consistent disparity values with <img width="69" height="16" align="BOTTOM" border="0" src="./Stereo matching_files/img6.gif" alt="$m \leftrightarrow m&#39;$">.</p><p>
Other criteria may also be used in the update process,
	e.g. the geometric support as proposed by
	Ayache and Faverjon (1987). 
</p></li></ul><h2><a name="SECTION00054000000000000000">
Coarse-to-fine multiresolution matching scheme</a>
</h2>
<a name="sec:c-to-f">&nbsp;</a>The coarse-to-fine multiresolution matching scheme
works as follows:
<ul>
<li> generate a pair of image pyramids (a hierarchy of images)
	from the original image pair so that
	only few and prominent features are
	present at the coarse levels.
	The original images are at the finest level
	of the image pyramids.
</li><li> start the matching process at the coarsest level.
</li><li> use the matches obtained at the coarser
	level to guide the matching process
	gradually up to the finest level.
</li></ul>
<p>
<br>
</p><div align="CENTER"><a name="fig:zc">&nbsp;</a><a name="209">&nbsp;</a>
<table>
<caption><strong>Figure 8:</strong>
By applying to an image  Laplacian of Gaussian filters
	of different <img width="14" height="15" align="BOTTOM" border="0" src="./Stereo matching_files/img30.gif" alt="$\sigma$"> values followed by a detection
	of zero-crossings, a hierarchy of edge map from
	coarse to fine levels can be obtained.</caption>
<tbody><tr><td><img width="470" height="516" src="./Stereo matching_files/img31.gif" alt="\begin{figure}
\begin{center}
\
\begin{minipage}
{5cm}
(a) original image \\ 
\p...
 ...{figure=lapgauss/cameraman6.ps,width=5cm}
\end{minipage}\end{center}\end{figure}"></td></tr>
</tbody></table>
</div>
<br>
<p>
An image pyramid of edges can be obtained by convolving the images
with a Laplacian of Gaussian filter of different widths (<img width="14" height="15" align="BOTTOM" border="0" src="./Stereo matching_files/img30.gif" alt="$\sigma$">)followed by a detection of zero-crossings (Fig.&nbsp;<a href="http://homepages.inf.ed.ac.uk/rbf/CVonline/LOCAL_COPIES/OWENS/LECT11/node5.html#fig:zc">8</a>).
An image pyramid of grey level images can be obtained
by applying a smoothing operation followed by sub-sampling
(i.e. reducing the image resolution by a factor of 2).
Such image pyramid is sometimes referred to as the <em>processing cone</em>
(Fig.&nbsp;<a href="http://homepages.inf.ed.ac.uk/rbf/CVonline/LOCAL_COPIES/OWENS/LECT11/node5.html#fig:proc-cone">9</a>).
The SRI STEREOSYS system described later used this
smooth and sub-sample scheme.
</p><p>
<br>
</p><div align="CENTER"><a name="fig:proc-cone">&nbsp;</a><a name="217">&nbsp;</a>
<table>
<caption><strong>Figure 9:</strong>
Processing cone.</caption>
<tbody><tr><td><img width="441" height="338" src="./Stereo matching_files/img32.gif" alt="\begin{figure}
\begin{center}
\

\psfig {figure=proc-cone.xfigps,height=7.5cm}
\end{center}\end{figure}"></td></tr>
</tbody></table>
</div>
<br><hr>
<!--Navigation Panel-->
<a name="tex2html98" href="http://homepages.inf.ed.ac.uk/rbf/CVonline/LOCAL_COPIES/OWENS/LECT11/node6.html">
<img width="37" height="24" align="BOTTOM" border="0" alt="next" src="./Stereo matching_files/next_motif.gif"></a> 
<a name="tex2html96" href="http://homepages.inf.ed.ac.uk/rbf/CVonline/LOCAL_COPIES/OWENS/LECT11/lect11.html">
<img width="26" height="24" align="BOTTOM" border="0" alt="up" src="./Stereo matching_files/up_motif.gif"></a> 
<a name="tex2html90" href="http://homepages.inf.ed.ac.uk/rbf/CVonline/LOCAL_COPIES/OWENS/LECT11/node4.html">
<img width="63" height="24" align="BOTTOM" border="0" alt="previous" src="./Stereo matching_files/previous_motif.gif"></a>   
<br>
<b> Next:</b> <a name="tex2html99" href="http://homepages.inf.ed.ac.uk/rbf/CVonline/LOCAL_COPIES/OWENS/LECT11/node6.html">Some existing matching algorithms</a>
<b> Up:</b> <a name="tex2html97" href="http://homepages.inf.ed.ac.uk/rbf/CVonline/LOCAL_COPIES/OWENS/LECT11/lect11.html">Computer Vision IT412</a>
<b> Previous:</b> <a name="tex2html91" href="http://homepages.inf.ed.ac.uk/rbf/CVonline/LOCAL_COPIES/OWENS/LECT11/node4.html">Problem definition</a>
<!--End of Navigation Panel-->
<address>
<a href="mailto:robyn@cs.uwa.edu.au">Robyn Owens</a>
<br> 
<i>10/29/1997</i>
</address>


</body></html>