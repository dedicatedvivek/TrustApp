\chapter{\textbf{Mathematics related to Belief Propagation Technique}}

\section{Probabilistic Formulation:}

Bayesian Methods provide a formalism for reasoning about partial beliefs under condition of uncertainty..
In the Bayesian formalism,beliefs measures obey the three basic axioms of probability theory.


\begin{equation*}
   0 \leq P(A) \leq 1
\end{equation*}

\begin{equation*}
    P(Sure Propositions)=1
\end{equation*}

\begin{equation*}
  P (A    or  B) = P(A)+ P(B)
\end{equation*}
 if A and B are mutually exclusive
\\The basic expressions in the Bayesian formalism are statement about conditional probabilities be that
the probability of any event A computed by conditioning it on any set of exhaustive and mutually exclusive events $B_i,i=1,2,......n$



\begin{equation*}
     P(A)= \sum \{P(A\setminus B_i)\times P(B_i)\}
\end{equation*}


This decomposition provides the basics for the hypothetical or assumption based reasoning in the bayesian formalism.It states that the belief in any event A is weighted sum over the beliefs in all the distinct  ways that event A might be realized.
\\ Another useful generalization of the product rule is so called \textbf{Chain Rule Formula}.
\begin{equation*}
P(A,B)=P(A|B)P(B)
\end{equation*}

It states that if  a set of n events $(E{_1},E{_2},......E{_n})$


then,the probability of joint event $(E_1,E{_2},......E{_n})$ can be write
ten as a product of n conditional probabilities.


\begin{equation*}
    P(E{_1},E{_2},......E{_n})=P(E_{n}|E_{n-1},......E{_2},E{_1})......P(E{_2}|E{_1})P(E{_1})
\end{equation*}
This product can be derived by repeated application of equation 2.5. in any convenient order.
\section{Probability Model}

A probability model is an encoding of the probabilistic information in some domain of interest,such that each well formed sentence,statement or proposition can be calculated in accordance with axioms of probability theory.
\\These sentences are represented by interrelating random or stochastic variables with in model.
\\The  probability model (M) is defined by the joint probability of all its variables or universe

%$\triangleq$

Each variable in a probabilistic model may take any one of a set of mutually exclusive or collectively
exhaustive states.
The random variable may be discrete or continuous.The discrete random variable has finite of possible states,the probability distribution of Discrete random variable is probability mass function whereas continuous random variable takes their state from an infinite set of possible values.The probability distribution of continuous random variable is probability density function.
\section{Probability Language}

Probability language is well suited to reasoning under uncertainty.It provide a suitable frame work for processing the uncertainty relationship between variables of a probabilistic model.
probability language provide consistency of inferred results and knowledge base through their axiomatic basis and provide a convenient mechanism for presenting uncertain results.
The basic four primitives of probability language are\begin{enumerate}
                                                      \item Likelihood
                                                      \item Conditioning
                                                      \item Relevance
                                                     \item Causation
                                                     \end{enumerate}

\begin{itemize}
  \item A likelihood of event  is measure of  how likely or probable it is that the event will occur ie.chance of occurrence.
  \item A conditioning :An event is conditional  second event ,if its  is changed by the knowledge of second event states.
  \item Relevance: events A and B are said to be relevant to each other in the context of event C ,if adding the knowledge of C to the knowledge of B,changes the likelihood of A.
  Example: Two events are relevance,when common sequences is observed.ie A and B are relevant if both are causes of C.
\item Causation conveys a pattern of dependence between events.
\end{itemize}

\section{Graphical Representation}
A probabilistic model is dependency model in which the relationship between each of the variable is captured.
A graph is denoted by G(V,E) is set of nodes or vertices V connected by the set of arcs or edge E.
Graphs may be used to represent dependency model
\\For example :Dependency Model M comprising the variables U represented by graph.
\\The set of arc E represents conditional dependencies between these variables.
\\ie.Joint probability function of M is encoded in E

%G $\triangleq$
%\triangleq {G{M}(U,E)}$
Nodes of the graph corresponds to variables in dependency model
U$\rightarrow$ variables
\paragraph\   Graphical representation for probabilistic model are two types one is undirected graph and directed graph.
Undirected graph had no explicit direction,an arc of influence between connected nodes
Examples of undirected graph is Markov Random Fields.
In directed graph arcs are either unidirectional or bidirectional directed arc provide the mechanism for representing causation
Example for Directed graph is Bayesian network
\paragraph{Markov Random Fields and Bayesian network are two types of graphical representation for probabilistic models}

\section{Bayesian Belief Network}
The Bayesian Belief Network(BBN)determines the state probabilities of each node or variable from predetermined conditional and prior probabilities.
The Direct Acyclic Graph(DAG) G(U,E)of the probability model M represents probability distribution P(U) ,where U represents set of all variables in M
\begin{equation}\label{}
   X{_1},X{_2},........X{_n}
\end{equation}
Variables in probability distribution P(U)
Direct Acyclic Graph(DAG) in which minimal set of variables are designed as parent of each variable
\X{i} such that
\begin{equation}\label{}
   P(X{_i}/W);   W\in{X{_1},X{_2}.....X{_{i-1}}}
\end{equation}

The above equation is BBN of that probability distribution
For DAG to be Bayesian network of M,it is necessary and sufficient that each variable be conditional independent of all of its non descendents given in parents also satisfies this condition.
\\\
In practice usually understands the constrains in the domain of interest.However easily identify the variables that directly influence other variables ie.eaiser to understand the local interaction of variables than the domain as a whole.
\\
The basic concept in the Bayesian treatment of uncertainties in causal network is conditional probability ie.
Given the event B,the probability of event A is x

\begin{equation}
    P(a / b) = x
\end{equation}
Conditional probability
\begin{equation}
    P(a/b)P(b)=P(a,b)
\end{equation}
In real world all events are conditioned by some context C=c
\begin{equation}
    P(a/b,c)P(b/c)=P(a,b/c)
\end{equation}
The Baye's rule also conditioned on
\begin{equation}
 P(b/a,c)={P(a/b,c)P(b/c)} / {P(a/c)}
\end{equation}
\begin{itemize}
  \item P(b/a,c) is posterior probability of b
  \item P(a/b,c) liklihood probability
  \item P(a/c) Normalized factor
  \item P(b/c) Priori probability of b
\end{itemize}
Normalised factor is not directly availableis replaced by
\begin{equation}\label{}
    \int_{b} P(a/b,c)p(b/c)db
\end{equation}
\begin{equation}
    \sum_{b}P(a/b,c)p(b/c)
\end{equation}

For continuous and discrete distributions


\section{Markov Field Belief Technique }
\ Markov  Random Field of Belief propagation Technique is used to convey information for useful decisions and for  inference problems.
\ Markov  Field of Belief propagation Technique can be expressed in terms of conditional independence of its non neighbors,once values of neighbors  are known.
\\ If assigning of weights to the links of the graph must be handled with caution.If the weights are to be used translating evidential data in to meaning full probabilistic inferences such probabilistic model is both consistent and complete.Consistency guarantees that,it don't over load the graph with too many parameters.


The theory of Markov fields provides a safe method for constructing a complete and consistent quantitative model while preserving the dependency structure for an arbitrary graph (G).\\


The method consists of four steps
\begin{enumerate}
  \item Identify the associates of G ,namely, the maximal subgraphs whose nodes are all adjacent to each other

% P(A)= \sum \{P(A\setminus $B_i$)\times P($B_i$)\}

   %\item For each associate $C{_i}$,assign a nonnegative compatibility function $g{_i}$$(c{_i}$ ,which measures the relative degree of compatibility associated with each value assignment c{$_i$}to the variable included in$ C{_i}$
  %\item Form the product \\prod{$_i$}g{$_i$}(c{$_i$}) of the compatibility functions over all associates.
 % \item Normalize the product of all possible value combinations of the variables of the system
\begin{equation}\label{}
    P( x{_i},....,x{_n}) =K\prod_{i}g{_i}(c{_i})
\end{equation}

where
 \begin{equation}\label{}
    K= 1\div[\sum_{x{_1},...x{_n}}\prod_{i}g{_i}(c{_i})]
 \end{equation}

\end{enumerate}












